# -*- coding: utf-8 -*-
"""MLModelForPeptidePrediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vvCd_FK73nuOmm6WZ__Ym8P-nxQ9cQmi

*   Dataset credit to AxPEP ([Paper](https://www.cell.com/molecular-therapy-family/nucleic-acids/fulltext/S2162-2531(20)))
*   Instruction credit to Dr. Chanin Natesenamat
"""

##Enabling the installation of CD-HIT (Function see below)
! wget https://repo.anaconda.com/miniconda/Miniconda3-py37_4.8.2-Linux-x86_64.sh
! chmod +x Miniconda3-py37_4.8.2-Linux-x86_64.sh
! bash ./Miniconda3-py37_4.8.2-Linux-x86_64.sh -b -f -p /usr/local
import sys
sys.path.append('/usr/local/lib/python3.7/site-packages/')

##Pfeature is used to compute the properties of amino acid, crucial to find the molecular properties of peptides. We will use the feature to construct our machine-learning model
##Peptides - Short stretches of amino acid chain
! wget https://github.com/raghavagps/Pfeature/raw/master/PyLib/Pfeature.zip

! unzip Pfeature.zip

# Commented out IPython magic to ensure Python compatibility.
# % cd Pfeature

! python setup.py install

##CD-HIT filters out any redundant peptides
! conda install -c bioconda cd-hit -y

##Loading the peptide dataset
##.fasta format is the ascii text file containing rows of peptide sequences
##pos - has anti-microvial activity
##neg - no anti-microbial activity
! wget https://raw.githubusercontent.com/dataprofessor/AMP/main/train_po.fasta
! wget https://raw.githubusercontent.com/dataprofessor/AMP/main/train_ne.fasta

! wget https://raw.githubusercontent.com/dataprofessor/AMP/main/train_ne.fasta

##standard protocol: removing redundant sequences having high peptide similarity greater than 0.99
! cd-hit -i train_po.fasta -o train_po_cdhit.txt -c 0.99

! cd-hit -i train_ne.fasta -o train_ne_cdhit.txt -c 0.99

!pip install lazypredict

import pandas as pd
from sklearn.feature_selection import VarianceThreshold
import lazypredict
from lazypredict.Supervised import LazyClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import matthews_corrcoef
from Pfeature.pfeature import aac_wp
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report

##Positive and negative datasets will be merged into the same dataset
##Binary classification using scikit-learn
##Define functions for calculating the different features
def aac(input):
  a = input.rstrip('txt')
  output = a + 'aac.csv'
  df_out = aac_wp(input, output)
  df_in = pd.read_csv(output)
  return df_in

aac('train_po_cdhit.txt')
pos = 'train_po_cdhit.txt'
neg = 'train_ne_cdhit.txt'
def feature_calc(po, ne, feature_name):
  # Calculate feature
  po_feature = feature_name(po)
  ne_feature = feature_name(ne)
  # Create class labels
  po_class = pd.Series(['positive' for i in range(len(po_feature))])
  ne_class = pd.Series(['negative' for i in range(len(ne_feature))])
  # Combine po and ne via pd.concat
  po_ne_class = pd.concat([po_class, ne_class], axis=0)
  po_ne_class.name = 'class'
  po_ne_feature = pd.concat([po_feature, ne_feature], axis=0)
  # Combine feature and class
  df = pd.concat([po_ne_feature, po_ne_class], axis=1)
  return df

feature = feature_calc(pos, neg, aac)
feature

##Data preprocessing
#Splitting X as feature and Y as class
X = feature.drop('class', axis=1) ##To drop class column
y = feature['class'].copy()

# Encoding the Y class label
y = y.map({"positive": 1, "negative": 0})

fs = VarianceThreshold(threshold=0.1)
fs.fit_transform(X)
#X2.shape
X2 = X.loc[:, fs.get_support()]
X2

X_train, X_test, y_train, y_test = train_test_split(X2, y, test_size=0.2, random_state =42, stratify=y)

clf = LazyClassifier(verbose=0,ignore_warnings=True, custom_metric=matthews_corrcoef)
models_train,predictions_train = clf.fit(X_train, X_train, y_train, y_train)

models_test, predictions_test=clf.fit(X_train, X_test, y_train, y_test)

##Plot of accuracy
import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(5, 10))
sns.set_theme(style="whitegrid")
ax = sns.barplot(y=models_train.index, x="Accuracy", data=models_train)
ax.set(xlim=(0, 1))

# Plot of MCC
import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(5, 10))
sns.set_theme(style="whitegrid")
ax = sns.barplot(y=models_train.index, x="matthews_corrcoef", data=models_train)
ax.set(xlim=(0, 1))

# Build random forest model

rf = RandomForestClassifier(n_estimators=500)

rf.fit(X_train, y_train)

y_train_pred = rf.predict(X_train)
y_test_pred = rf.predict(X_test)

# Simplest and quickest way to obtain the model performance (Accuracy)
rf.score(X_test,y_test)

# Matthew Correlation Coefficient
matthews_corrcoef(y_test, y_test_pred)

confusion_matrix(y_test, y_test_pred)

# ROC curve
import matplotlib.pyplot as plt
from sklearn.metrics import plot_roc_curve

plot_roc_curve(rf, X_test, y_test)  
plt.show()